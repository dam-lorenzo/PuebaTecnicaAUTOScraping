Decidi empezar el scraping omitiendo la arte de cada sucursal, de momento solo me centrare en traer todos los items disponibles, y armar el csv
Luego de analizar la pagina por un rato, encontre una API que provee todas las categorias, a partir de esta api puedo obtener un slug el cual me servira para una api de productos, la cual me lista todos los items y que puedo ir recorriendo con un paginador.
En principio tengo que armar una clase para hacer las peticiones a la api, en una primer instancia va a constar solo de una sesion de requests, voy a armar un metodo para actualizar los headers de ser necesario (para las dos APIs que tengo de momento no hace falta ninguno) la idea es que esta clase tenga un metodo que haga las peticiones teniendo en cuenta cualquier excepcion de requests. Tambien voy a agregar la opcion de devolver un json o un texto, por si mas adelante necesito usar bs4 poder reutilizar el metodo facilmente.

Decidi crear una clase payload para poder armar el formato de cada producto de forma mas facil, tambien cree una clase llamada PayloadKeys en la cual voy a centralizar el nombre de cada columna del csv.
Del lado del recorrido de la api fue bastante simple ya que contaba con un indice de inicio y fin para la cantidad de productos a traer, deje los valores que usa la pagina por defecto a pesar de que en las pruebas podia traer de a mas cantidad, igualmente esto se puede cambiar facilmente desde el archivo url.py que unifica todos los endpoints de la api y formateo de la misma.
De momento el crawler funciona perfecto (omitiendo la parte de las sucursales), trae todos los productos sin problemas y los plasma en un csv. El proximo paso es ver como funciona la seleccion de sucursal para poder agregar esto.
