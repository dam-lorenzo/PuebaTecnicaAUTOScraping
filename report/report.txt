Decidi empezar el scraping omitiendo la arte de cada sucursal, de momento solo me centrare en traer todos los items disponibles, y armar el csv
Luego de analizar la pagina por un rato, encontre una API que provee todas las categorias, a partir de esta api puedo obtener un slug el cual me servira para una api de productos, la cual me lista todos los items y que puedo ir recorriendo con un paginador.
En principio tengo que armar una clase para hacer las peticiones a la api, en una primer instancia va a constar solo de una sesion de requests, voy a armar un metodo para actualizar los headers de ser necesario (para las dos APIs que tengo de momento no hace falta ninguno) la idea es que esta clase tenga un metodo que haga las peticiones teniendo en cuenta cualquier excepcion de requests. Tambien voy a agregar la opcion de devolver un json o un texto, por si mas adelante necesito usar bs4 poder reutilizar el metodo facilmente.

Decidi crear una clase payload para poder armar el formato de cada producto de forma mas facil, tambien cree una clase llamada PayloadKeys en la cual voy a centralizar el nombre de cada columna del csv.
Del lado del recorrido de la api fue bastante simple ya que contaba con un indice de inicio y fin para la cantidad de productos a traer, deje los valores que usa la pagina por defecto a pesar de que en las pruebas podia traer de a mas cantidad, igualmente esto se puede cambiar facilmente desde el archivo url.py que unifica todos los endpoints de la api y formateo de la misma.
De momento el crawler funciona perfecto (omitiendo la parte de las sucursales), trae todos los productos sin problemas y los plasma en un csv. El proximo paso es ver como funciona la seleccion de sucursal para poder agregar esto.

Viendo como funciona la seleccion de sucursal, encontre un json embebido en el html de la pagina el cual contiene la informacion de todas las sucursales disponibles, incluso tienen su id solo me faltaria ver en que endpoint, o en que header va ese id. Para obtener este json voy a agregar un metodo al request_handler que me permita obtener el html parseado a bs4, porque el tag html tiene un id por el cual buscarlo.

Estoy terminando de hacer algunas pruebas, creo que el cambio de sucursal se hace a traves de las cookies (tambien esta el parametro sc en la url de la api, creo que deberia sacarlo, este usa el indice de la sucursal en la lista del json, me parece un tanto confuso) pero modificando este paramentro y pasando el id de la sucursal se haria el cambio. Haciendo pruebas con postman todavia no termino de asegurarme de que vaya a funcionar

Despues de varias pruebas con distintas combinaciones de endpoint y headers, consegui hacer el cambio de sucursal usando los metodos que mencione antes, tanto la Cookie, como el indice de sucursal en la url de la api. Ya tendria todo el crawler listo voy a dejarlo correr para hacer un analisis de la metadata que se esta trayendo a ver si no hay que ajustar algo
